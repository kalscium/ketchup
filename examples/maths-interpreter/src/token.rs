//! Lexer and token definitions

use std::fmt::Debug;
use logos::{Logos, SpannedIter};
use crate::{error::Error, span::{Span, Spanned}};

/// A token generated by the lexer
#[derive(Debug, Clone, Logos, PartialEq)]
#[logos(skip r"[ \t\r\n\f]+")] // whitespace
#[logos(skip r"\/\/[^\n\r]*")] // `//` comments
#[logos(skip r"#[^\n\r]*")] // `#` comments
#[logos(skip r"\/\*[^\*\/]*\*\/")] // multi-line comments
pub enum Token {
    // Literals
    /// A number literal
    #[regex(r"[0-9]+", |lex| lex.slice().parse::<f64>().unwrap())]
    #[regex(r"[0-9]+\.[0-9]+f", |lex| lex.slice()[1..lex.slice().len()-1].parse::<f64>().unwrap())]
    Number(f64),

    // Symbols
    //
    // note: meaning shouldn't be encoded at this point, that's the parser's job
    #[token("+")]
    Plus,
    #[token("-")]
    Dash,
    #[token("*")]
    Star,
    #[token("/")]
    Slash,

    // Parentheses
    #[token("(")]
    LParen,
    #[token(")")]
    RParen,
}

/// An (optional) next token
pub type NextTok = Option<Spanned<Token>>;

/// A value that also includes the next (optional) token
#[derive(Debug, Clone)]
pub struct NextTokWith<T: Debug + Clone> {
    pub item: Spanned<T>,
    pub next_tok: NextTok,
}

/// Wraps the next (optional) token from the lexer to be more ergonomic and clean
pub fn next_token(
    filename: &str,
    tokens: &mut SpannedIter<'_, Token>,
) -> Result<Option<Spanned<Token>>, Error> {
    let next_token = tokens.next();
    match next_token {
        Some((Ok(token), span)) => Ok(Some(Spanned::new(
            token,
            Span { filename: filename.to_string(), range: span },
        ))),
        Some((Err(()), span)) => Err(Error::UnexpectedCharacter(
            Span { filename: filename.to_string(), range: span },
        )),
        None => Ok(None),
    }
}
